{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec 구현 프로젝트 \n",
        "\n",
        "Efficient Estimation of Word Representations in Vector Space \n",
        "\n",
        "논문의 word2vec의 skip-gram 방식을 pytorch를 이용해 구현하고 학습하는 과정을 담았습니다. "
      ],
      "metadata": {
        "id": "45zkWhslef6q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:29:36.641276Z",
          "start_time": "2022-02-19T14:29:36.638642Z"
        },
        "id": "HlEy3xfY4WSh"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:50:41.644583Z",
          "start_time": "2022-02-19T12:50:41.642937Z"
        },
        "id": "cBrr7-gt4jnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a08e69-c1f8-4fe1-8b2c-8674d4df8007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 40.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.4\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:26:59.276355Z",
          "start_time": "2022-02-19T14:26:58.411434Z"
        },
        "id": "6mC9lhsJ4WSh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:05.586472Z",
          "start_time": "2022-02-19T14:30:05.583611Z"
        },
        "id": "17g7UZ5g4WSi"
      },
      "outputs": [],
      "source": [
        "# seed\n",
        "seed = 7777\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:06.721039Z",
          "start_time": "2022-02-19T14:30:06.717559Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3UlC7Jn4WSi",
        "outputId": "cf6c6998-752c-422f-9009-f61e4a71ddf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla T4\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHs8_LU04WSj"
      },
      "source": [
        "### 토크나이징이 완료된 위키 백과 코퍼스 다운로드 및 불용어 사전 크롤링\n",
        "[데이터 다운로드 출처](https://ratsgo.github.io/embedding/downloaddata.html)\n",
        " \n",
        "[불용어 사전 출처](https://www.ranks.nl/stopwords/korean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CGhLvEQECmWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d1da1a-6b40-47cb-aa00-34ee5f7f7bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "JYD_WjPYCmWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18aabebf-8283-4b3b-f338-0ca286eee1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/nlp/w2v\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/nlp/w2v "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:11.886643Z",
          "start_time": "2022-02-19T14:27:11.884858Z"
        },
        "id": "4QPBJ6UZ4WSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936e8827-51be-4e23-a44c-d7ee475b37a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.5.18.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1Ybp_DmzNEpsBrUKZ1-NoPDzCMO39f-fx\n",
            "To: /content/drive/MyDrive/nlp/w2v/tokenized.zip\n",
            "100% 873M/873M [00:06<00:00, 139MB/s]\n",
            "Archive:  tokenized.zip\n",
            "   creating: tokenized/\n",
            "  inflating: tokenized/korquad_mecab.txt  \n",
            "  inflating: tokenized/wiki_ko_mecab.txt  \n",
            "  inflating: tokenized/corpus_mecab_jamo.txt  \n",
            "  inflating: tokenized/ratings_okt.txt  \n",
            "  inflating: tokenized/ratings_khaiii.txt  \n",
            "  inflating: tokenized/ratings_hannanum.txt  \n",
            "  inflating: tokenized/ratings_soynlp.txt  \n",
            "  inflating: tokenized/ratings_mecab.txt  \n",
            "  inflating: tokenized/ratings_komoran.txt  \n"
          ]
        }
      ],
      "source": [
        "# 데이터 다운로드\n",
        "!pip install gdown\n",
        "!gdown https://drive.google.com/u/0/uc?id=1Ybp_DmzNEpsBrUKZ1-NoPDzCMO39f-fx\n",
        "!unzip tokenized.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.633947Z",
          "start_time": "2022-02-19T14:27:13.829982Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTHCHmO24WSj",
        "outputId": "5382b556-34b7-4e59-ad60-4eff25571f37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Korean stop words: 677\n"
          ]
        }
      ],
      "source": [
        "# 한국어 불용어 리스트 크롤링\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.ranks.nl/stopwords/korean\"\n",
        "response = requests.get(url, verify = False)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text,'html.parser')\n",
        "    content = soup.select_one('#article178ebefbfb1b165454ec9f168f545239 > div.panel-body > table > tbody > tr')\n",
        "    stop_words=[]\n",
        "    for x in content.strings:\n",
        "        x=x.strip()\n",
        "        if x:\n",
        "            stop_words.append(x)\n",
        "    print(f\"# Korean stop words: {len(stop_words)}\")\n",
        "else:\n",
        "    print(response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.642775Z",
          "start_time": "2022-02-19T14:27:15.635333Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3d0IqhDF4WSk",
        "outputId": "adb61787-d61d-4f46-cc4a-b9d67a55ca45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'아'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "stop_words[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t76Q1pQ4WSk"
      },
      "source": [
        "### 단어 사전 구축 함수 구현 \n",
        "- 문서 리스트를 입력 받아 사전을 생성하는 함수 \n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - docs : 문서 리스트\n",
        "        - min_count : 최소 단어 등장 빈도수 (단어 빈도가 `min_count` 미만인 단어는 사전에 포함하지 않음)\n",
        "\n",
        "    - 반환값 \n",
        "        - word2count : 단어별 빈도 사전 (key: 단어, value: 등장 횟수)\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 (key: 단어 인덱스(int), value: 단어)\n",
        "        - word2wid : 인덱스(wid)별 단어 사전 (key: 단어, value: 단어 인덱스(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:01.637431Z",
          "start_time": "2022-02-19T14:32:56.730711Z"
        },
        "id": "xkjqztIA4WSl"
      },
      "outputs": [],
      "source": [
        "# 코퍼스 로드\n",
        "_DATA_DIR=\"/content/drive/MyDrive/nlp/w2v/tokenized\"\n",
        "\n",
        "with open(os.path.join(_DATA_DIR, \"wiki_ko_mecab.txt\")) as reader:\n",
        "    docs = reader.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:03.423002Z",
          "start_time": "2022-02-19T14:33:03.419818Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAKB6bbt4WSl",
        "outputId": "46795e34-866b-41d8-e20e-fb4e86ac63a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# wiki documents: 311,237\n"
          ]
        }
      ],
      "source": [
        "print(f\"# wiki documents: {len(docs):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:04.016885Z",
          "start_time": "2022-02-19T14:33:03.962269Z"
        },
        "id": "-OI1MCXv4WSl",
        "outputId": "1798759a-f2e7-46a8-81f9-d1ce5db0f348",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# 500개로 문서 개수를 줄임\n",
        "docs=random.sample(docs,500)\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 영어, 특수문자 제거 \n",
        "docs = [re.compile('[ㄱ-ㅣ가-힣]+').findall(str(doc)) for doc in docs]\n",
        "print(f\"Check : {docs[0][:1000]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fa9LmkFhpyy",
        "outputId": "a784fa7d-ce91-4509-c605-c4c43d8e44ed"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check : ['남모', '공주', '는', '신라', '의', '공주', '왕족', '으로', '법흥왕', '과', '보과', '공주', '부여', '씨', '의', '딸', '이', '며', '백제', '동성왕', '의', '외손녀', '였', '다', '경쟁자', '인', '준정', '과', '함께', '신라', '의', '초대', '여성', '원화', '화랑', '였', '다', '그', '가', '준정', '에게', '암살', '당한', '것', '을', '계기', '로', '화랑', '은', '여성', '이', '아닌', '남성', '미소년', '으로', '선발', '하', '게', '되', '었', '다', '신라', '진흥왕', '에게', '는', '사촌', '누나', '이', '자', '이모', '가', '된다', '신라', '의', '청소년', '조직', '이', '었', '던', '화랑도', '는', '처음', '에', '는', '남모', '준정', '두', '미녀', '를', '뽑', '아', '이', '를', '원화', '라', '했으며', '이', '들', '주위', '에', '는', '여', '명', '의', '무리', '를', '따르', '게', '하', '였', '다', '그러나', '준정', '과', '남모', '는', '서로', '최고', '가', '되', '고자', '시기', '하', '였', '다', '준정', '은', '박영실', '을', '섬겼', '는데', '지소태후', '는', '자신', '의', '두', '번', '째', '남편', '이', '기', '도', '한', '그', '를', '싫어해서', '준정', '의', '원화', '를', '없애', '고', '낭도', '가', '부족', '한', '남모', '에게', '위화랑', '의', '낭도', '를', '더', '해', '주', '었', '다', '그', '뒤', '남모', '는', '준정', '의', '초대', '로', '그', '의', '집', '에', '갔', '다가', '억지로', '권하', '는', '술', '을', '받아마시', '고', '취한', '뒤', '준정', '에', '의해', '강물', '에', '던져져', '살해', '되', '었', '다', '이', '일', '이', '발각', '돼', '준', '정도', '사형', '에', '처해지', '고', '나라', '에서', '는', '귀족', '출신', '의', '잘', '생기', '고', '품행', '이', '곧', '은', '남자', '를', '뽑', '아', '곱', '게', '단장', '한', '후', '이', '를', '화랑', '이', '라', '칭하', '고', '받들', '게', '하', '였', '다', '부왕', '신라', '제', '대', '국왕', '법흥왕', '모후', '보과', '공주', '부여', '씨', '공주', '남모', '공주', '외조부', '백제', '제', '대', '국왕', '동성왕', '외조모', '신라', '이찬', '비지', '의', '딸', '화랑전사', '마루', '년', '배우', '박효빈', '신라', '법흥왕', '백제', '동성왕', '준정', '화랑', '분류', '년', '죽음', '분류', '신라', '의', '왕녀', '분류', '신라', '의', '왕족', '분류', '화랑', '분류', '암살', '된', '사람', '분류', '독살', '된', '사람', '분류', '법흥왕']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:27.904880Z",
          "start_time": "2022-02-19T14:33:27.899620Z"
        },
        "id": "OAkkQsvO4WSl"
      },
      "outputs": [],
      "source": [
        "def make_vocab(docs:List[str], min_count:int):\n",
        "    \"\"\"\n",
        "    'docs'문서 리스트를 입력 받아 단어 사전을 생성.\n",
        "    \n",
        "    return \n",
        "        - word2count : 단어별 빈도 사전\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 \n",
        "        - word2wid : 인덱스(wid)별 단어 사전\n",
        "    \"\"\"\n",
        "\n",
        "    word2count = dict()\n",
        "    word2id = dict()\n",
        "    id2word = dict()\n",
        "\n",
        "    _word2count = dict() # 단어별 등장 빈도를 기록하기 위한 임시 딕셔너리 생성\n",
        "    for doc in tqdm(docs):\n",
        "        # 단어 개수가 3개 이하는 skip\n",
        "        if len(doc)>3:\n",
        "            for word in doc:\n",
        "                # 불용어 리스트에 포함된 단어 제거\n",
        "                if word in stop_words:\n",
        "                    continue\n",
        "                # 임시 딕셔너리에 단어별 등장 빈도 기록 \n",
        "                try:\n",
        "                    _word2count[word]+=1\n",
        "                except KeyError:\n",
        "                    _word2count[word]=1\n",
        "\n",
        "    # 토큰 최소 빈도를 만족하는 토큰만 사전(word2count)에 추가\n",
        "    idx=0\n",
        "    for w,c in _word2count.items():\n",
        "        if c<min_count:\n",
        "            continue\n",
        "        word2count[w] = c\n",
        "        word2id[w] = idx\n",
        "        id2word[idx] = w\n",
        "        idx+=1\n",
        "\n",
        "    \n",
        "    return word2count, word2id, id2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.157872Z",
          "start_time": "2022-02-19T14:33:28.473330Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieS5SiQx4WSm",
        "outputId": "9786b0cf-9d3d-47eb-96fe-0149bbdc4c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:02<00:00, 201.94it/s]\n"
          ]
        }
      ],
      "source": [
        "word2count, word2id, id2word = make_vocab(docs, min_count=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.754722Z",
          "start_time": "2022-02-19T14:33:30.752115Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT1MRN1EJtx6",
        "outputId": "b0a5d071-d75f-4e69-8ff2-bc32d967704d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161,255\n"
          ]
        }
      ],
      "source": [
        "doc_len = sum(word2count.values()) # 문서 내 모든 단어의 개수 (unique한 단어 개수 * 단어 등장 빈도)\n",
        "print(f\"{doc_len:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:32.916830Z",
          "start_time": "2022-02-19T14:33:32.914355Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_1MneB54WSm",
        "outputId": "880b401b-ff40-4a42-9cf7-86c28969c84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# unique word : 5,661\n"
          ]
        }
      ],
      "source": [
        "print(f\"# unique word : {len(word2id):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHxtZqtk4WSm"
      },
      "source": [
        "### Dataset 클래스 구현\n",
        "- Skip-Gram 방식의 학습 데이터 셋(`Tuple(target_word, context_word)`)을 생성하는 `CustomDataset` \n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - docs: 문서 리스트\n",
        "        - word2id: 단어별 인덱스(wid) 사전\n",
        "        - window_size: Skip-Gram의 윈도우 사이즈\n",
        "    - 메소드\n",
        "        - `make_pair()`\n",
        "            - 문서를 단어로 쪼개고, 사전에 존재하는 단어들만 단어 인덱스로 변경\n",
        "            - Skip-gram 방식의 `(target_word, context_word)` 페어(tuple)들을 `pairs` 리스트에 담아 반환\n",
        "        - `__len__()`\n",
        "            - `pairs` 리스트의 개수 반환\n",
        "        - `__getitem__(index)`\n",
        "            - `pairs` 리스트를 인덱싱"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.111290Z",
          "start_time": "2022-02-19T14:33:38.104531Z"
        },
        "id": "UPiLcYCZ4WSm"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, docs:List[str], word2id:Dict[str,int], window_size:int=5):\n",
        "        self.docs = docs\n",
        "        self.word2id = word2id\n",
        "        self.window_size = window_size\n",
        "        self.pairs = self.make_pair()\n",
        "    \n",
        "    def make_pair(self):\n",
        "        \"\"\"\n",
        "        (target, context) 형식의 Skip-gram pair 데이터 셋 생성 \n",
        "        \"\"\"\n",
        "        pairs = []\n",
        "        for doc in tqdm(self.docs):\n",
        "            word_ids = [] # 문서 내 (사전에 존재하는) 단어의 인덱스를 저장하기 위한 리스트 \n",
        "            for word in doc:\n",
        "                # 사전(\bword2id)에 존재하는 단어만 단어 인덱스(wid)로 변경해 학습 데이터에 추가\n",
        "                if self.word2id.get(word):\n",
        "                    word_ids.append(self.word2id.get(word))\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            # 학습 데이터 구축\n",
        "            for i, u in enumerate(word_ids):\n",
        "                for j in range(self.window_size):\n",
        "                    if i-1-j>=0: # target_word 기준 왼쪽 방향의 context_word들을 추가\n",
        "                        pairs.append((u, word_ids[i-1-j]))\n",
        "                    if i+1+j<len(word_ids): # target_word 기준 오른쪽 방향의 context_word들을 추가\n",
        "                        pairs.append((u, word_ids[i+1+j]))\n",
        "        return pairs\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.945361Z",
          "start_time": "2022-02-19T14:33:38.385577Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YntOw2q94WSm",
        "outputId": "020bc842-204a-497a-dbd4-3c755244836e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 696.52it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = CustomDataset(docs, word2id, window_size=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.949614Z",
          "start_time": "2022-02-19T14:33:38.946663Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RpNbAjk4WSn",
        "outputId": "6ee55243-ce68-4406-86cd-6491a5c896b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1597490"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:43.072635Z",
          "start_time": "2022-02-19T14:33:43.069526Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FBwcL4H4WSn",
        "outputId": "7a92d9ff-c2f4-4390-b558-14276eb4f7ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:51.040595Z",
          "start_time": "2022-02-19T14:33:51.031473Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTAwTjKk4WSn",
        "outputId": "8380f167-deb9-44eb-cf46-fdc6a087abfa",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(공주, 는)\n",
            "(공주, 신라)\n",
            "(공주, 공주)\n",
            "(공주, 왕족)\n",
            "(공주, 공주)\n",
            "(는, 공주)\n",
            "(는, 신라)\n",
            "(는, 공주)\n",
            "(는, 왕족)\n",
            "(는, 공주)\n",
            "(는, 부여)\n",
            "(신라, 는)\n",
            "(신라, 공주)\n",
            "(신라, 공주)\n",
            "(신라, 왕족)\n",
            "(신라, 공주)\n",
            "(신라, 부여)\n",
            "(신라, 씨)\n",
            "(공주, 신라)\n",
            "(공주, 왕족)\n",
            "(공주, 는)\n",
            "(공주, 공주)\n",
            "(공주, 공주)\n",
            "(공주, 부여)\n",
            "(공주, 씨)\n",
            "(공주, 딸)\n",
            "(왕족, 공주)\n",
            "(왕족, 공주)\n",
            "(왕족, 신라)\n",
            "(왕족, 부여)\n",
            "(왕족, 는)\n",
            "(왕족, 씨)\n",
            "(왕족, 공주)\n",
            "(왕족, 딸)\n",
            "(왕족, 며)\n",
            "(공주, 왕족)\n",
            "(공주, 부여)\n",
            "(공주, 공주)\n",
            "(공주, 씨)\n",
            "(공주, 신라)\n",
            "(공주, 딸)\n",
            "(공주, 는)\n",
            "(공주, 며)\n",
            "(공주, 공주)\n",
            "(공주, 백제)\n",
            "(부여, 공주)\n",
            "(부여, 씨)\n",
            "(부여, 왕족)\n",
            "(부여, 딸)\n",
            "(부여, 공주)\n",
            "(부여, 며)\n",
            "(부여, 신라)\n",
            "(부여, 백제)\n",
            "(부여, 는)\n",
            "(부여, 였)\n",
            "(씨, 부여)\n",
            "(씨, 딸)\n",
            "(씨, 공주)\n",
            "(씨, 며)\n",
            "(씨, 왕족)\n",
            "(씨, 백제)\n",
            "(씨, 공주)\n",
            "(씨, 였)\n",
            "(씨, 신라)\n",
            "(씨, 다)\n",
            "(딸, 씨)\n",
            "(딸, 며)\n",
            "(딸, 부여)\n",
            "(딸, 백제)\n",
            "(딸, 공주)\n",
            "(딸, 였)\n",
            "(딸, 왕족)\n",
            "(딸, 다)\n",
            "(딸, 공주)\n",
            "(딸, 인)\n",
            "(며, 딸)\n",
            "(며, 백제)\n",
            "(며, 씨)\n",
            "(며, 였)\n",
            "(며, 부여)\n",
            "(며, 다)\n",
            "(며, 공주)\n",
            "(며, 인)\n",
            "(며, 왕족)\n",
            "(며, 준정)\n",
            "(백제, 며)\n",
            "(백제, 였)\n",
            "(백제, 딸)\n",
            "(백제, 다)\n",
            "(백제, 씨)\n",
            "(백제, 인)\n",
            "(백제, 부여)\n",
            "(백제, 준정)\n",
            "(백제, 공주)\n",
            "(백제, 신라)\n",
            "(였, 백제)\n",
            "(였, 다)\n",
            "(였, 며)\n",
            "(였, 인)\n",
            "(였, 딸)\n"
          ]
        }
      ],
      "source": [
        "# verify (target word, context word)\n",
        "for i, pair in enumerate(dataset):\n",
        "    if i==100:\n",
        "        break\n",
        "    print(f\"({id2word[pair[0]]}, {id2word[pair[1]]})\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Z50-Dr4WSn"
      },
      "source": [
        "### 위에서 생성한 `dataset`으로 DataLoader  객체 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.645176Z",
          "start_time": "2022-02-19T14:34:02.642780Z"
        },
        "id": "GXcAvFB14WSn"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    dataset, batch_size=64, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.777322Z",
          "start_time": "2022-02-19T14:34:02.774335Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Yfcwi_14WSn",
        "outputId": "e95108a0-acfd-4152-e2cf-7ea9620f84cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24961"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTs16gsU4WSn"
      },
      "source": [
        "### Negative Sampling 함수 구현\n",
        "- Skip-Gram은 복잡도를 줄이기 위한 방법으로 negative sampling을 사용한다. \n",
        "[negative distribution 설명](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#How-are-negative-samples-drawn?)\n",
        "\n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - batch_size : 배치 사이즈, matrix의 row 개수 \n",
        "        - n_neg_sample : negative sample의 개수, matrix의 column 개수\n",
        "    - 반환값 \n",
        "        - neg_v : 추출된 negative sample (2차원의 리스트)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.397509Z",
          "start_time": "2022-02-19T14:34:11.386389Z"
        },
        "id": "PUqIB6dH4WSn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# negative sample을 추출할 sample table 생성 \n",
        "sample_table = []\n",
        "sample_table_size = doc_len\n",
        "\n",
        "# noise distribution 생성\n",
        "alpha = 3/4\n",
        "frequency_list = np.array(list(word2count.values())) ** alpha\n",
        "Z = sum(frequency_list)\n",
        "ratio = frequency_list/Z\n",
        "negative_sample_dist = np.round(ratio*sample_table_size)\n",
        "\n",
        "for wid, c in enumerate(negative_sample_dist):\n",
        "    sample_table.extend([wid]*int(c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.508414Z",
          "start_time": "2022-02-19T14:34:11.505464Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdu8qK8x4WSn",
        "outputId": "bd613150-c2e9-401a-947f-951a14b7f3bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "161091"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "len(sample_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.656046Z",
          "start_time": "2022-02-19T14:34:11.653325Z"
        },
        "id": "mQIVrOIR4WSn"
      },
      "outputs": [],
      "source": [
        "def get_neg_v_negative_sampling(batch_size:int, n_neg_sample:int):\n",
        "    \"\"\"\n",
        "    (batch_size, n_neg_sample) shape의 네거티브 샘플 메트릭스 생성\n",
        "    \"\"\"\n",
        "    neg_v = np.random.choice(\n",
        "        sample_table, size=(batch_size, n_neg_sample)\n",
        "    ).tolist()\n",
        "    return neg_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:12.345976Z",
          "start_time": "2022-02-19T14:34:12.333448Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wwT4Af04WSo",
        "outputId": "ca683195-29ef-47d4-e6e8-a0da76ea1565"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[36, 4422, 52, 2801, 2018],\n",
              " [5559, 307, 4189, 371, 5444],\n",
              " [3480, 4774, 1336, 271, 180],\n",
              " [57, 3460, 1688, 4855, 1636]]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "get_neg_v_negative_sampling(4, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5UubCzK4WSo"
      },
      "source": [
        "### 미니 튜토리얼\n",
        "Skip-Gram 모델의 `forward` 및 `loss` 연산 방식\n",
        "- Reference\n",
        "\n",
        "    - [Skip-Gram negative sampling loss function 설명 영문 블로그](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#Derivation-of-Cost-Function-in-Negative-Sampling)\n",
        "    - [Skip-Gram negative sampling loss function 설명 한글 블로그](https://reniew.github.io/22/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:46.954048Z",
          "start_time": "2022-02-19T12:51:46.951529Z"
        },
        "id": "IAR68hsY4WSo"
      },
      "outputs": [],
      "source": [
        "# hyper parameter example\n",
        "emb_size = 30000 # vocab size\n",
        "emb_dimension = 300 # word embedding 차원\n",
        "n_neg_sample = 5\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.340056Z",
          "start_time": "2022-02-19T12:51:47.300999Z"
        },
        "id": "zzOsVUn94WSo"
      },
      "outputs": [],
      "source": [
        "# 1. Embedding Matrix와 Context Matrix를 생성\n",
        "u_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)\n",
        "v_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.352240Z",
          "start_time": "2022-02-19T12:51:49.341437Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7J_ADc44WSo",
        "outputId": "f8bffd04-b269-43ee-8b3f-f35febc456e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target word idx : tensor([24460, 10634,  2864, 23952,  3320, 15187, 19625, 26546, 27339,  3920,\n",
            "        25847,  6023,  5055,  7070,  6291, 10245, 15926,   641, 20178,  4565,\n",
            "         4784, 26715, 16955, 28742, 17947, 19774,  8065, 22605,  3061, 28965,\n",
            "         3056, 17963]) Pos context word idx : tensor([23224,  5636, 23712,  5234,  3991, 17897, 25123, 17938, 19634, 24228,\n",
            "          693,   799, 25457,  1308, 28935, 25696,  5601, 23878,  8312,  1292,\n",
            "        21380, 16974,  9318,  9578, 12915, 29271, 26465, 20572,  2362, 25929,\n",
            "        19754, 29080]) Neg context word idx : [[869, 3542, 900, 92, 629], [1015, 4851, 5519, 4237, 1502], [4712, 625, 1569, 762, 1984], [519, 1872, 3984, 1537, 362], [56, 3899, 2549, 14, 5260], [1033, 549, 683, 4227, 623], [2917, 1330, 1041, 2769, 2579], [705, 3040, 4881, 93, 4960], [3878, 285, 115, 2513, 331], [1093, 2828, 539, 114, 3726], [1486, 753, 1632, 4724, 5037], [872, 1911, 3143, 465, 1338], [3285, 346, 1813, 470, 87], [621, 4215, 2547, 4086, 52], [1646, 139, 5060, 545, 2122], [4608, 1084, 4003, 4432, 1004], [2887, 1088, 96, 1435, 888], [3575, 1970, 92, 1687, 331], [1438, 4164, 1698, 4881, 130], [2500, 623, 2164, 4175, 3003], [2397, 1564, 1224, 5630, 1516], [3663, 84, 4881, 1202, 86], [31, 3459, 1892, 222, 4962], [1914, 982, 5022, 5463, 804], [4458, 2342, 1297, 949, 5380], [1621, 3523, 3036, 5106, 1588], [3222, 139, 2011, 3208, 1857], [47, 941, 3697, 5583, 4244], [139, 5003, 285, 2660, 3018], [5247, 2570, 230, 1472, 2622], [23, 2626, 3532, 1040, 3143], [4391, 5407, 5187, 324, 128]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. wid(단어 인덱스)를 임의로 생성\n",
        "pos_u = torch.randint(high = emb_size, size = (batch_size,))\n",
        "pos_v = torch.randint(high = emb_size, size = (batch_size,))\n",
        "neg_v = get_neg_v_negative_sampling(batch_size, n_neg_sample)\n",
        "print(f\"Target word idx : {pos_u} Pos context word idx : {pos_v} Neg context word idx : {neg_v}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.364020Z",
          "start_time": "2022-02-19T12:51:49.353486Z"
        },
        "id": "4iEG0nCZ4WSo"
      },
      "outputs": [],
      "source": [
        "# 3. tensor로 변환\n",
        "pos_u = Variable(torch.LongTensor(pos_u)).to(device)\n",
        "pos_v = Variable(torch.LongTensor(pos_v)).to(device)\n",
        "neg_v = Variable(torch.LongTensor(neg_v)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:51.391896Z",
          "start_time": "2022-02-19T12:51:51.387084Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqbNbajG4WSo",
        "outputId": "219fbfda-f9ff-4f6b-ec35-059cc85629b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of pos_u embedding : torch.Size([32, 300])\n",
            " shape of pos_v embedding : torch.Size([32, 300])\n",
            " shape of neg_v embedding : torch.Size([32, 5, 300])\n"
          ]
        }
      ],
      "source": [
        "# 4. wid로 각각의 embedding matrix에서 word embedding 값을 가져오기\n",
        "pos_u = u_embedding(pos_u)\n",
        "pos_v = v_embedding(pos_v)\n",
        "neg_v = v_embedding(neg_v)\n",
        "print(f\"shape of pos_u embedding : {pos_u.shape}\\n shape of pos_v embedding : {pos_v.shape}\\n shape of neg_v embedding : {neg_v.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.121477Z",
          "start_time": "2022-02-19T12:51:52.646148Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDWUrSwo4WSo",
        "outputId": "f85fc2bf-e051-4672-c71c-bcd48c3ae482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 300])\n",
            "shape of pos logits : torch.Size([32])\n",
            "\n",
            "torch.Size([32, 5, 300])\n",
            "torch.Size([32, 300])\n",
            "torch.Size([32, 300, 1])\n",
            "torch.Size([32, 5, 1])\n",
            "shape of logits : torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "# 5. dot product \n",
        "pos_score = torch.mul(pos_u, pos_v) # 행렬 element-wise 곱 (= row 곱 )\n",
        "print(pos_score.shape)\n",
        "pos_score = torch.sum(pos_score, dim=1)\n",
        "print(f\"shape of pos logits : {pos_score.shape}\\n\")\n",
        "\n",
        "print(neg_v.shape) # 3d tensor (b,n,m)\n",
        "print(pos_u.shape)\n",
        "print(pos_u.unsqueeze(dim=2).shape) # last axis에 1차원을 추가 (b,m,p)\n",
        "neg_score = torch.bmm(neg_v, pos_u.unsqueeze(dim=2)) # batch-matrix-matrix multiplication output (b,n,p)\n",
        "print(neg_score.shape)\n",
        "neg_score = neg_score.squeeze() \n",
        "print(f\"shape of logits : {neg_score.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.670418Z",
          "start_time": "2022-02-19T12:51:53.665671Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adOpcoL54WSo",
        "outputId": "c433db81-ba2b-435b-ad44-c91e9ccd56c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos logits : -241.4199676513672\n",
            "neg logits : -1307.4857177734375\n",
            "Loss : 1548.9056396484375\n"
          ]
        }
      ],
      "source": [
        "# 6. loss 구하기\n",
        "pos_score = F.logsigmoid(pos_score)\n",
        "neg_score = F.logsigmoid(-1*neg_score) # negative의 logit은 minimize 하기 위해 -1 곱함\n",
        "print(f\"pos logits : {pos_score.sum()}\")\n",
        "print(f\"neg logits : {neg_score.sum()}\")\n",
        "loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "print(f\"Loss : {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muEceOGZ4WSo"
      },
      "source": [
        "### Skip-gram 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `SkipGram` 클래스를 구현\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `device` : 연산 장치 종류\n",
        "    - 생성자에서 생성해야할 변수 \n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `u_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (target_word)\n",
        "        - `v_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (context_word)\n",
        "    - 메소드\n",
        "        - `init_embedding()`\n",
        "            - 엠베딩 메트릭스 값을 초기화\n",
        "        - `forward()`\n",
        "            - 위 튜토리얼과 같이 dot product를 수행한 후 score를 생성\n",
        "            - loss를 반환 (loss 설명 추가)\n",
        "        - `save_emedding()`\n",
        "            - `u_embedding`의 단어 엠베딩 값을 단어 별로 파일에 저장\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:15.731306Z",
          "start_time": "2022-02-19T14:34:15.721129Z"
        },
        "id": "pnmMamP44WSo"
      },
      "outputs": [],
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size:int, emb_dimension:int, device:str):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embedding = nn.Embedding(vocab_size, emb_dimension, sparse=True).to(device)\n",
        "        self.v_embedding = nn.Embedding(vocab_size, emb_dimension, sparse=True).to(device)\n",
        "        self.init_embedding()\n",
        "    \n",
        "    \n",
        "    def init_embedding(self):\n",
        "        \"\"\"\n",
        "        u_embedding과 v_embedding 메트릭스 값을 초기화\n",
        "        \"\"\"\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        self.u_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embedding.weight.data.uniform_(-0, 0)\n",
        "    \n",
        "    \n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \"\"\"\n",
        "        dot product를 수행한 후 score를 생성\n",
        "        loss 반환\n",
        "        \"\"\"    \n",
        "        # 각각의 embedding matrix에서 word embedding 값을 가져오기\n",
        "        pos_u = self.u_embedding(pos_u)\n",
        "        pos_v = self.v_embedding(pos_v)\n",
        "        neg_v = self.v_embedding(neg_v)\n",
        "        \n",
        "        # dot product\n",
        "        pos_score = torch.mul(pos_u, pos_v)\n",
        "        pos_score = torch.sum(pos_score, dim=1)\n",
        "\n",
        "        # loss 구하기 \n",
        "        pos_score = F.logsigmoid(pos_score)\n",
        "        neg_score = torch.bmm(neg_v, pos_u.unsqueeze(dim=2)).squeeze()\n",
        "        neg_score = F.logsigmoid(-1 * neg_score) # negative의 logit은 minimize 하기 위해 -1 곱함\n",
        "        \n",
        "        loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "        return loss\n",
        "    \n",
        "    def save_embedding(self, id2word, file_name, use_cuda):\n",
        "        \"\"\"\n",
        "        'file_name' 위치에 word와 word_embedding을 line-by로 저장\n",
        "        파일의 첫 줄은 '단어 개수' 그리고 '단어 embedding 사이즈' 값을 입력해야 함\n",
        "        \"\"\"\n",
        "        if use_cuda: # parameter를 gpu 메모리에서 cpu 메모리로 옮김\n",
        "            embedding = self.u_embedding.weight.cpu().data.numpy()\n",
        "        else:\n",
        "            embedding = self.u_embedding.weight.data.numpy()\n",
        "        \n",
        "        with open(file_name, 'w') as writer:\n",
        "            # 파일의 첫 줄은 '단어 개수' 그리고 '단어 embedding 사이즈' 값을 입력해야 함\n",
        "            writer.write(f\"{len(id2word)} {embedding.shape[-1]}\\n\")\n",
        "            \n",
        "            for wid, word in id2word.items():\n",
        "                e = embedding[wid]\n",
        "                e = \" \".join([str(e_) for e_ in e])\n",
        "                writer.write(f\"{word} {e}\\n\")\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSWd5gV24WSp"
      },
      "source": [
        "### Skip-Gram 방식의  Word2Vec 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `Word2Vec` 클래스를 구현\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()`) 입력 매개 변수\n",
        "        - `input_file` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `device` : 연상 장치 종류\n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `min_count` : 사전에 추가될 단어의 최소 등장 빈도\n",
        "    - 생성자에서 생성해야 할 변수 \n",
        "        - `docs` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `word2count`, `word2id`, `id2word` : 위에서 구현한 `make_vocab()` 함수의 반환 값\n",
        "        - `device` : 연산 장치 종류\n",
        "        - `emb_size` : vocab의 (unique한) 단어 종류 \n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `model` : `SkipGram` 클래스의 인스턴스\n",
        "        - `optimizer` : `SGD` 클래스의 인스턴스\n",
        "    - 메소드\n",
        "        - `train()`\n",
        "            - 입력 매개변수 \n",
        "                - `train_dataloader`\n",
        "            - Iteration 횟수만큼 input_file 학습 데이터를 학습한다. 매 epoch마다 for loop 돌면서 batch 단위 학습 데이터를 skip gram 모델에 학습함. 학습이 끝나면 word embedding을 output_file_name 파일에 저장.\n",
        "- Reference\n",
        "    - [Optimizer - SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:20.503555Z",
          "start_time": "2022-02-19T14:34:20.491585Z"
        },
        "id": "Td-GQrqI4WSp"
      },
      "outputs": [],
      "source": [
        "class Word2Vec:\n",
        "    def __init__(self, \n",
        "                input_file: List[str],\n",
        "                output_file_name: str,\n",
        "                 device: str,\n",
        "                 emb_dimension=300,\n",
        "                 batch_size = 64,\n",
        "                 window_size=5,\n",
        "                 n_neg_sample = 5,\n",
        "                 iteration=1,\n",
        "                 lr = 0.02,\n",
        "                 min_count=5):\n",
        "        self.docs = input_file\n",
        "        self.output_file_name = output_file_name\n",
        "        self.word2count, self.word2id, self.id2word = make_vocab(self.docs, min_count=min_count)\n",
        "        self.device = device\n",
        "        self.emb_size = len(self.word2id)\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.batch_size =batch_size\n",
        "        self.window_size = window_size\n",
        "        self.n_neg_sample = n_neg_sample\n",
        "        self.iteration = iteration\n",
        "        self.lr = lr\n",
        "        self.model = SkipGram(self.emb_size, self.emb_dimension, self.device)\n",
        "        self.optimizer = SGD(\n",
        "            self.model.parameters(),\n",
        "            lr = self.lr\n",
        "        )\n",
        "        # train() 함수에서 만든 임베딩 결과 파일들을 저장할 폴더 생성\n",
        "        os.makedirs(self.output_file_name, exist_ok=True)\n",
        "        \n",
        "    \n",
        "    def train(self, train_dataloader):\n",
        "        \n",
        "        # lr 값을 조절하는 스케줄러 인스턴스 변수를 생성\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps=len(train_dataloader)*self.iteration\n",
        "        )\n",
        "        \n",
        "        for epoch in range(self.iteration):\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "            print(f\"*****Epoch {epoch} Total Step {len(train_dataloader)}*****\")\n",
        "            total_loss, batch_loss, batch_step = 0,0,0\n",
        "\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                batch_step+=1\n",
        "\n",
        "                pos_u, pos_v = batch\n",
        "                # negative data 생성\n",
        "                neg_v = get_neg_v_negative_sampling(pos_u.shape[0], self.n_neg_sample)\n",
        "                \n",
        "                # 데이터를 tensor화 & device 설정\n",
        "                pos_u = Variable(torch.LongTensor(pos_u)).to(self.device)\n",
        "                pos_v = Variable(torch.LongTensor(pos_v)).to(self.device)\n",
        "                neg_v = Variable(torch.LongTensor(neg_v)).to(self.device)\n",
        "\n",
        "                # gradient 초기화\n",
        "                self.optimizer.zero_grad()\n",
        "                self.model.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                loss = self.model.forward(pos_u, pos_v, neg_v)\n",
        "\n",
        "                # loss\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "\n",
        "                batch_loss += loss.item()\n",
        "                total_loss += loss.item()\n",
        "                \n",
        "                if (step%500 == 0) and (step!=0):\n",
        "                    print(f\"Step: {step} Loss: {batch_loss/batch_step:.4f} lr: {self.optimizer.param_groups[0]['lr']:.4f}\")\n",
        "                    # 변수 초기화    \n",
        "                    batch_loss, batch_step = 0,0\n",
        "            \n",
        "            print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "            print(f\"*****Epoch {epoch} Train Finished*****\\n\")\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Saving Embedding...*****\")\n",
        "            self.model.save_embedding(self.id2word, os.path.join(self.output_file_name, f'w2v_{epoch}.txt'), True if 'cuda' in self.device.type else False)\n",
        "            print(f\"*****Epoch {epoch} Embedding Saved at {os.path.join(self.output_file_name, f'w2v_{epoch}.txt')}*****\\n\")\n",
        "                    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:29.561892Z",
          "start_time": "2022-02-19T14:34:26.103659Z"
        },
        "id": "Ywx9R8n24WSp",
        "outputId": "9fb4b2e7-9ae7-450c-c471-890736cb6d61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:02<00:00, 203.86it/s]\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec 클래스의 인스턴스 생성\n",
        "output_file = os.path.join(\".\", \"word2vec_wiki\")\n",
        "w2v = Word2Vec(docs, output_file, device, n_neg_sample=10, iteration=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:34.615469Z",
          "start_time": "2022-02-19T14:34:34.055502Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufBxjKxN4WSp",
        "outputId": "cc3093af-1860-4045-c002-0d4bbd3fcdf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 697.81it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24961"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "# 학습 데이터 셋 및 데이터 로더 생성\n",
        "dataset = CustomDataset(w2v.docs, w2v.word2id, w2v.window_size)\n",
        "train_dataloader = DataLoader(dataset, w2v.batch_size)\n",
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:45:38.362817Z",
          "start_time": "2022-02-19T14:34:37.382371Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JBUrUJ34WSp",
        "outputId": "21c4a979-c774-4f50-9de9-b0483141ddb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Epoch 0 Train Start*****\n",
            "*****Epoch 0 Total Step 24961*****\n",
            "Step: 500 Loss: 483.4350 lr: 0.0199\n",
            "Step: 1000 Loss: 381.9070 lr: 0.0197\n",
            "Step: 1500 Loss: 310.7652 lr: 0.0196\n",
            "Step: 2000 Loss: 259.5921 lr: 0.0195\n",
            "Step: 2500 Loss: 251.3580 lr: 0.0193\n",
            "Step: 3000 Loss: 205.1873 lr: 0.0192\n",
            "Step: 3500 Loss: 165.6209 lr: 0.0191\n",
            "Step: 4000 Loss: 219.0585 lr: 0.0189\n",
            "Step: 4500 Loss: 210.9005 lr: 0.0188\n",
            "Step: 5000 Loss: 200.9764 lr: 0.0187\n",
            "Step: 5500 Loss: 192.6000 lr: 0.0185\n",
            "Step: 6000 Loss: 196.7885 lr: 0.0184\n",
            "Step: 6500 Loss: 194.8715 lr: 0.0183\n",
            "Step: 7000 Loss: 209.8101 lr: 0.0181\n",
            "Step: 7500 Loss: 207.7619 lr: 0.0180\n",
            "Step: 8000 Loss: 196.4875 lr: 0.0179\n",
            "Step: 8500 Loss: 191.3960 lr: 0.0177\n",
            "Step: 9000 Loss: 191.1824 lr: 0.0176\n",
            "Step: 9500 Loss: 159.0769 lr: 0.0175\n",
            "Step: 10000 Loss: 114.8233 lr: 0.0173\n",
            "Step: 10500 Loss: 187.4499 lr: 0.0172\n",
            "Step: 11000 Loss: 190.4287 lr: 0.0171\n",
            "Step: 11500 Loss: 197.7998 lr: 0.0169\n",
            "Step: 12000 Loss: 157.9021 lr: 0.0168\n",
            "Step: 12500 Loss: 195.7434 lr: 0.0167\n",
            "Step: 13000 Loss: 153.1389 lr: 0.0165\n",
            "Step: 13500 Loss: 99.8602 lr: 0.0164\n",
            "Step: 14000 Loss: 111.3485 lr: 0.0163\n",
            "Step: 14500 Loss: 199.7804 lr: 0.0161\n",
            "Step: 15000 Loss: 186.7503 lr: 0.0160\n",
            "Step: 15500 Loss: 206.3853 lr: 0.0159\n",
            "Step: 16000 Loss: 185.2412 lr: 0.0157\n",
            "Step: 16500 Loss: 180.5285 lr: 0.0156\n",
            "Step: 17000 Loss: 192.9554 lr: 0.0155\n",
            "Step: 17500 Loss: 201.9167 lr: 0.0153\n",
            "Step: 18000 Loss: 164.1769 lr: 0.0152\n",
            "Step: 18500 Loss: 173.8581 lr: 0.0151\n",
            "Step: 19000 Loss: 177.9600 lr: 0.0149\n",
            "Step: 19500 Loss: 198.3662 lr: 0.0148\n",
            "Step: 20000 Loss: 193.6624 lr: 0.0147\n",
            "Step: 20500 Loss: 197.8878 lr: 0.0145\n",
            "Step: 21000 Loss: 185.8651 lr: 0.0144\n",
            "Step: 21500 Loss: 180.2667 lr: 0.0143\n",
            "Step: 22000 Loss: 193.5450 lr: 0.0141\n",
            "Step: 22500 Loss: 189.3389 lr: 0.0140\n",
            "Step: 23000 Loss: 188.1360 lr: 0.0139\n",
            "Step: 23500 Loss: 193.9490 lr: 0.0137\n",
            "Step: 24000 Loss: 190.4518 lr: 0.0136\n",
            "Step: 24500 Loss: 200.2574 lr: 0.0135\n",
            "Epoch 0 Total Mean Loss : 200.1568\n",
            "*****Epoch 0 Train Finished*****\n",
            "\n",
            "*****Epoch 0 Saving Embedding...*****\n",
            "*****Epoch 0 Embedding Saved at ./word2vec_wiki/w2v_0.txt*****\n",
            "\n",
            "*****Epoch 1 Train Start*****\n",
            "*****Epoch 1 Total Step 24961*****\n",
            "Step: 500 Loss: 190.7156 lr: 0.0132\n",
            "Step: 1000 Loss: 186.1779 lr: 0.0131\n",
            "Step: 1500 Loss: 191.0327 lr: 0.0129\n",
            "Step: 2000 Loss: 183.0147 lr: 0.0128\n",
            "Step: 2500 Loss: 191.5531 lr: 0.0127\n",
            "Step: 3000 Loss: 168.8639 lr: 0.0125\n",
            "Step: 3500 Loss: 151.6396 lr: 0.0124\n",
            "Step: 4000 Loss: 190.0043 lr: 0.0123\n",
            "Step: 4500 Loss: 186.5851 lr: 0.0121\n",
            "Step: 5000 Loss: 184.4665 lr: 0.0120\n",
            "Step: 5500 Loss: 181.9803 lr: 0.0119\n",
            "Step: 6000 Loss: 180.2236 lr: 0.0117\n",
            "Step: 6500 Loss: 186.2773 lr: 0.0116\n",
            "Step: 7000 Loss: 195.1870 lr: 0.0115\n",
            "Step: 7500 Loss: 190.9486 lr: 0.0113\n",
            "Step: 8000 Loss: 189.1233 lr: 0.0112\n",
            "Step: 8500 Loss: 185.7559 lr: 0.0111\n",
            "Step: 9000 Loss: 183.4930 lr: 0.0109\n",
            "Step: 9500 Loss: 160.1810 lr: 0.0108\n",
            "Step: 10000 Loss: 117.2412 lr: 0.0107\n",
            "Step: 10500 Loss: 187.6846 lr: 0.0105\n",
            "Step: 11000 Loss: 190.5229 lr: 0.0104\n",
            "Step: 11500 Loss: 191.4163 lr: 0.0103\n",
            "Step: 12000 Loss: 152.8243 lr: 0.0101\n",
            "Step: 12500 Loss: 187.2445 lr: 0.0100\n",
            "Step: 13000 Loss: 142.8914 lr: 0.0099\n",
            "Step: 13500 Loss: 99.8367 lr: 0.0097\n",
            "Step: 14000 Loss: 113.0618 lr: 0.0096\n",
            "Step: 14500 Loss: 193.5693 lr: 0.0095\n",
            "Step: 15000 Loss: 187.2012 lr: 0.0093\n",
            "Step: 15500 Loss: 202.8869 lr: 0.0092\n",
            "Step: 16000 Loss: 187.0872 lr: 0.0091\n",
            "Step: 16500 Loss: 182.8397 lr: 0.0089\n",
            "Step: 17000 Loss: 192.7633 lr: 0.0088\n",
            "Step: 17500 Loss: 197.3883 lr: 0.0087\n",
            "Step: 18000 Loss: 173.7246 lr: 0.0085\n",
            "Step: 18500 Loss: 178.4406 lr: 0.0084\n",
            "Step: 19000 Loss: 179.5652 lr: 0.0083\n",
            "Step: 19500 Loss: 196.1959 lr: 0.0081\n",
            "Step: 20000 Loss: 195.7083 lr: 0.0080\n",
            "Step: 20500 Loss: 196.7742 lr: 0.0079\n",
            "Step: 21000 Loss: 188.9126 lr: 0.0077\n",
            "Step: 21500 Loss: 182.2654 lr: 0.0076\n",
            "Step: 22000 Loss: 191.7282 lr: 0.0075\n",
            "Step: 22500 Loss: 189.5903 lr: 0.0073\n",
            "Step: 23000 Loss: 189.3199 lr: 0.0072\n",
            "Step: 23500 Loss: 192.1770 lr: 0.0071\n",
            "Step: 24000 Loss: 189.8298 lr: 0.0069\n",
            "Step: 24500 Loss: 196.5895 lr: 0.0068\n",
            "Epoch 1 Total Mean Loss : 180.6192\n",
            "*****Epoch 1 Train Finished*****\n",
            "\n",
            "*****Epoch 1 Saving Embedding...*****\n",
            "*****Epoch 1 Embedding Saved at ./word2vec_wiki/w2v_1.txt*****\n",
            "\n",
            "*****Epoch 2 Train Start*****\n",
            "*****Epoch 2 Total Step 24961*****\n",
            "Step: 500 Loss: 191.3861 lr: 0.0065\n",
            "Step: 1000 Loss: 189.9442 lr: 0.0064\n",
            "Step: 1500 Loss: 193.7398 lr: 0.0063\n",
            "Step: 2000 Loss: 186.5403 lr: 0.0061\n",
            "Step: 2500 Loss: 193.2707 lr: 0.0060\n",
            "Step: 3000 Loss: 173.1264 lr: 0.0059\n",
            "Step: 3500 Loss: 159.2813 lr: 0.0057\n",
            "Step: 4000 Loss: 193.3658 lr: 0.0056\n",
            "Step: 4500 Loss: 193.9904 lr: 0.0055\n",
            "Step: 5000 Loss: 190.0229 lr: 0.0053\n",
            "Step: 5500 Loss: 189.2027 lr: 0.0052\n",
            "Step: 6000 Loss: 183.4701 lr: 0.0051\n",
            "Step: 6500 Loss: 192.7452 lr: 0.0049\n",
            "Step: 7000 Loss: 197.1057 lr: 0.0048\n",
            "Step: 7500 Loss: 194.3320 lr: 0.0047\n",
            "Step: 8000 Loss: 193.4009 lr: 0.0045\n",
            "Step: 8500 Loss: 189.3294 lr: 0.0044\n",
            "Step: 9000 Loss: 187.1121 lr: 0.0043\n",
            "Step: 9500 Loss: 173.8855 lr: 0.0041\n",
            "Step: 10000 Loss: 130.2002 lr: 0.0040\n",
            "Step: 10500 Loss: 195.5199 lr: 0.0039\n",
            "Step: 11000 Loss: 195.2756 lr: 0.0037\n",
            "Step: 11500 Loss: 194.0175 lr: 0.0036\n",
            "Step: 12000 Loss: 155.5335 lr: 0.0035\n",
            "Step: 12500 Loss: 184.8875 lr: 0.0033\n",
            "Step: 13000 Loss: 143.0163 lr: 0.0032\n",
            "Step: 13500 Loss: 107.6113 lr: 0.0031\n",
            "Step: 14000 Loss: 123.5261 lr: 0.0029\n",
            "Step: 14500 Loss: 196.8701 lr: 0.0028\n",
            "Step: 15000 Loss: 194.0805 lr: 0.0027\n",
            "Step: 15500 Loss: 203.0673 lr: 0.0025\n",
            "Step: 16000 Loss: 196.8965 lr: 0.0024\n",
            "Step: 16500 Loss: 191.0097 lr: 0.0023\n",
            "Step: 17000 Loss: 195.9134 lr: 0.0021\n",
            "Step: 17500 Loss: 206.8341 lr: 0.0020\n",
            "Step: 18000 Loss: 194.5554 lr: 0.0019\n",
            "Step: 18500 Loss: 196.3920 lr: 0.0017\n",
            "Step: 19000 Loss: 190.9368 lr: 0.0016\n",
            "Step: 19500 Loss: 197.8494 lr: 0.0015\n",
            "Step: 20000 Loss: 206.2599 lr: 0.0013\n",
            "Step: 20500 Loss: 198.5786 lr: 0.0012\n",
            "Step: 21000 Loss: 199.0522 lr: 0.0011\n",
            "Step: 21500 Loss: 193.6300 lr: 0.0009\n",
            "Step: 22000 Loss: 195.4675 lr: 0.0008\n",
            "Step: 22500 Loss: 195.1489 lr: 0.0007\n",
            "Step: 23000 Loss: 194.9530 lr: 0.0005\n",
            "Step: 23500 Loss: 195.2328 lr: 0.0004\n",
            "Step: 24000 Loss: 196.0715 lr: 0.0003\n",
            "Step: 24500 Loss: 203.1136 lr: 0.0001\n",
            "Epoch 2 Total Mean Loss : 186.6128\n",
            "*****Epoch 2 Train Finished*****\n",
            "\n",
            "*****Epoch 2 Saving Embedding...*****\n",
            "*****Epoch 2 Embedding Saved at ./word2vec_wiki/w2v_2.txt*****\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 학습 \n",
        "w2v.train(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 유사도 계산\n",
        "\n",
        "학습을 통해 얻어낸 임베딩으로 유사도 계산해보기"
      ],
      "metadata": {
        "id": "KZ35BaFnvstf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "3oJAdMdPu4CY"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format('./word2vec_wiki/w2v_2.txt', binary=False)"
      ],
      "metadata": {
        "id": "1xftAm9Mu5JN"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar(positive=['대통령'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9ygDTJKvO0M",
        "outputId": "bea7abb5-501c-48f8-9b1f-b50d8decfa9f"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('국회의원', 0.9721125960350037),\n",
              " ('당', 0.9688351154327393),\n",
              " ('민주', 0.966854453086853),\n",
              " ('장관', 0.9648937582969666),\n",
              " ('학회', 0.9629717469215393),\n",
              " ('국민', 0.96233069896698),\n",
              " ('해군', 0.9616174697875977),\n",
              " ('영조', 0.9608238935470581),\n",
              " ('변호사', 0.9607787132263184),\n",
              " ('연합', 0.9583690166473389)]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar(positive=['서울'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66PW5vnPvSIb",
        "outputId": "441fd39e-b28b-4f8a-a6a1-eed8eb7d634f"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('현대', 0.9180622100830078),\n",
              " ('랑', 0.9111118912696838),\n",
              " ('화랑', 0.9018165469169617),\n",
              " ('갤러리', 0.901596188545227),\n",
              " ('국제', 0.9004174470901489),\n",
              " ('부산', 0.8989148139953613),\n",
              " ('미술관', 0.8983471393585205),\n",
              " ('아트', 0.8968197107315063),\n",
              " ('샘터', 0.8910189867019653),\n",
              " ('페어', 0.8865504264831543)]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar(positive=['남자'], negative=['여자'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4QiEQn-vXzv",
        "outputId": "abb43d0c-25f6-485f-aed0-457da69e37a5"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('쓰', 0.5390145778656006),\n",
              " ('용어', 0.5344403982162476),\n",
              " ('못하', 0.5001216530799866),\n",
              " ('알려', 0.49944472312927246),\n",
              " ('는다', 0.49281448125839233),\n",
              " ('기술', 0.4882909953594208),\n",
              " ('싶', 0.4847915470600128),\n",
              " ('형성', 0.48163795471191406),\n",
              " ('소프트웨어', 0.47975271940231323),\n",
              " ('못했', 0.4789581000804901)]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}